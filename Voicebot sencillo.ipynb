{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de Voice Bot informativo implementado al 100% en Python\n",
    "\n",
    "### Es un ejemplo sencillo de voicebot que emplea librerías para el reconocimiento y la síntesis de voz.\n",
    "\n",
    "### Te recomiendo que si no estás familiarizado con estas soluciones, primero revises el ejemplo de chatbot sencillo. Este notebook está basado en él"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El voicebot informa a los usuarios acerca de las normas de un crucero. Es un ejemplo básico, pero que bien sirve de ejemplo de uso de lematización y búsqueda de coincidencias entre las preguntas de usuario y las diferentes respuestas posibles mediante el modelo \"cosine_similarity\"\n",
    "\n",
    "#### Resumen técnico.\n",
    "\n",
    "##### 1.- En una variable de texto se almacena el corpus (diferentes respuestas posibles al usuario).\n",
    "##### 2.- Cuando el usuario plantea una pregunta, se agrega -temporalmente- al final de la lista de respuestas. A todo este contenido se le eliminan signos de puntuación, se tokeniza, lematiza y se extraen sus caracterísaticas -mediante TfidfVectorizer de sklearn-. A partir de ellas y empleando un modelo del tipo \"cosine_similarity\" se buscan las respuestas más coincidentes con la pregunta del usuario, se elige la que mayor grado de coincidentcia muestra y se responde con ella.\n",
    "##### 3.- Adicionalmente se ha incluido un pequeño módulo de saludo inicial y de despedida, que aleatoriamente elige una respuesta entre varias posibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('punkt') # Instalar módulo punkt si no está ya instalado (solo ejecutar la primera vez)\n",
    "#nltk.download('wordnet') # Instalar módulo wordnet si no está ya instalado (solo ejecutar la primera vez)\n",
    "\n",
    "import speech_recognition as sr # librería para reconocimiento de voz\n",
    "import pyttsx3 # librería para síntesis de voz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1 Carga del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debe modificarse para que indique exactamente la ubicación del archivo que contiene el corpus. \n",
    "# Puede sustiruirse por un corpus sobrecualquier otra temática\n",
    "f=open(r'C:\\Corpus_crucero.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "#print(raw)  #Si deseas ver el corpus, descomenta esta línea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Definición de funciones y variables de apoyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=raw.lower() # Convertimos todo el texto a minúsculas, para evitar deficiencias en la extracción de características\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw) # Convierte el corpus a una lista de sentencias\n",
    "word_tokens = nltk.word_tokenize(raw) # Convierte el corpus a una lista de palabras\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer() # Instanciamos el lematizador, con el que convertir las palabras  a sus raíces contextuales\n",
    "\n",
    "#LemTokens es una función que lematiza todos los tokens que se le pasan como parámetro\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# remove_punct es un diccionario del tipo (0signo de puntuación', None), que se emplea en la función\n",
    "# LemNormalize para sustituir los signos de puntuación por \"nada\" es decir, eliminarlos.\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# Dado un texto como parámetro, elimina los signos de puntuación, lo convierte a minúsculas,\n",
    "# lo tokeniza -por palabras- y finalmente lo lematiza\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Preprocesamiento del texto y evaluación de la similitud entre el mensaje de usuario y las respuestas definidas en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para determinar la similitud del texto insertado y el corpus\n",
    "def response(user_response):\n",
    "    \n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response) # Añade al final del corpus la respuesta de usuario. Se hace esto para que al usasr cosine_similarity las shapes de pregunta y respuestas coincidan y no de error\n",
    "    \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words('spanish'))\n",
    "    \n",
    "    caract_textos = TfidfVec.fit_transform(sent_tokens)\n",
    "    \n",
    "    # Ahora vamos a evaluar la similitud y devolver la respuesta adecuada a partir del corpus\n",
    "    \n",
    "    # vals es un vector con los grados de coincidencia entre la pregunta y los textos.\n",
    "    # la última línea de los textos es una pregunta, por lo que en ese caso será siempre 1, nos interesa el inmediato siguiente\n",
    "    vals = cosine_similarity(caract_textos[-1], caract_textos) \n",
    "    \n",
    "    # argsort ordena en orden creciente el vector, pero devuelve los índices originales, no los valores, por lo que\n",
    "    # en idx tendremos ekl índice del término que más se ajusta como respuesta.\n",
    "    idx=vals.argsort()[0][-2] # Se ordena el vector de coincidencias (ascendente) y se toma el índice del penúltimo -el último es la pregunta de usuario-, que es el de mayor coincidencia\n",
    "    \n",
    "    flat = vals.flatten()  # eliminamos una de las dimensiones convirtiendo vals en vector\n",
    "    flat.sort()\n",
    "    nivel_coincidencia = flat[-2] # obtenemos el nivel de coincidencia para saber si hay una respuesta válida\n",
    "    \n",
    "    if(nivel_coincidencia==0):\n",
    "        robo_response=robo_response+\"Lo siento, no te he entendido. Si no puedo responder a lo que busca póngase en contacto con atención al cliente en el 902.902.902 (llamada local)\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Definición de funcionalidades de saludo y despedida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saludo_inputs = (\"hola\", \"buenas\", \"saludos\", \"qué tal\", \"hey\",\"buenos dias\",)\n",
    "saludo_outputs = [\"Hola\", \"Hola, ¿Qué tal?\", \"Hola, ¿Cómo te puedo ayudar?\", \"Hola, encantado de hablar contigo\"]\n",
    "\n",
    "\n",
    "despedidas = [\"Nos vemos\", \"Hasta pronto\", \"Nos vemos en otro rato\", \"Chao\", \"Bye\", \"Un placer, espero haberte ayudado\"]\n",
    "\n",
    "def saludos(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in saludo_inputs:\n",
    "            return random.choice(saludo_outputs)\n",
    "        \n",
    "def despedida():\n",
    "    return random.choice(despedidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 Configuración del reconocedor de voz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se instancia el reconocedor en 'r'\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Si queremos interpretar el audio desde un fichero\n",
    "#audio_file = sr.AudioFile('audio_ejemplo.wav') # El fichero debe estar en la ruta del intérprete de Python\n",
    "\n",
    "# si en el fichero de audio hay mucho y solo nos interesa una parte...\n",
    "#with audio_file as source:\n",
    "#    r.adjust_for_ambient_noise(source) # Calibración si el fichero tiene ruido de fondo\n",
    "#    audio = r.record(source) # si solo se quiere una porción del audio: audio = r.record(source, offset=4, duration=3)\n",
    "\n",
    "# Si audio desde micrófono\n",
    "mic = sr.Microphone()\n",
    "\n",
    "# Si se necesita seleccionar un micrófono que no sea el de por defecto\n",
    "#sr.Microphone.list_microphone_names()\n",
    "#mic = sr.Microphone(device_index=1) #1 es el micro por defecto,  6 sería para el micrófono 6 del listado\n",
    "\n",
    "# Definición de la función que va a encargarse de reconocer la voz del usuario y convertirla a texto\n",
    "def reconocer_voz():\n",
    "    retorno = ''\n",
    "    while (retorno==''):\n",
    "        try:\n",
    "            with mic as source:\n",
    "                r.adjust_for_ambient_noise(source) # Calibración si el audio tiene ruido de fondo\n",
    "                audio = r.listen(source)\n",
    "            retorno = r.recognize_google(audio, language='es-ES') # Definimos la API a utilizar como método de reconocimiento, la única que no necesita login es la de Google (debemos estar conectados a internet)\n",
    "        except:\n",
    "            retorno = ''\n",
    "    return retorno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6 Configuración del sistema de síntesis de voz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instancia del sistema de síntesis de voz\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "engine.setProperty('rate', 180)    # Aquí puedes seleccionar la velocidad de la voz\n",
    "engine.setProperty('voice', 'spanish') # configuración\n",
    "\n",
    "#definición de la función que reproduce una respuesta del voice bot\n",
    "def habla(texto):\n",
    "    engine.say(texto)\n",
    "    engine.runAndWait()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 Bucle conversacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "#print(\"ROBOT: Mi nombre es ROBOT. Contestaré a tus preguntas acerca de sus vacaciones en el crucero. Si quieres salir, solamente di 'salir' \")\n",
    "habla(\"Mi nombre es ROBOT. Contestaré a tus preguntas acerca de sus vacaciones en el crucero. Si quieres salir, solamente diga 'salir' \")\n",
    "while(flag==True):\n",
    "    #user_response = input()\n",
    "    user_response = reconocer_voz()\n",
    "    #print(user_response)\n",
    "    user_response = user_response.lower() #Convertimos a minúscula\n",
    "    \n",
    "    if(user_response!='salir'):\n",
    "        \n",
    "        if(user_response=='gracias' or user_response=='muchas gracias'): #Se podría haber definido otra función de coincidencia manual\n",
    "            flag=True\n",
    "            habla(\"No hay de qué\")\n",
    "            \n",
    "        else:\n",
    "            if(saludos(user_response)!=None): #Si la palabra insertada por el usuario es un saludo (Coincidencias manuales definidas previamente)\n",
    "                habla(saludos(user_response))\n",
    "                \n",
    "            else: #Si la palabra insertada no es un saludo --> CORPUS\n",
    "                #print(\"ROBOT: \",end=\"\") \n",
    "                habla(response(user_response))\n",
    "                sent_tokens.remove(user_response) # para eliminar del corpus la respuesta del usuario y volver a evaluar con el CORPUS limpio\n",
    "    else:\n",
    "        flag=False\n",
    "        habla(despedida())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
